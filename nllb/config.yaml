#################################################
# onmt_build_vocab options:
#################################################

## Where the samples will be written
save_data: samples
## Where the vocab(s) will be written
src_vocab: 'vocab/src'
tgt_vocab: 'vocab/tgt'
# Prevent overwriting existing files in the folder
overwrite: False

# NOTE Use sentencepiece even if SPM model is BPE, since this just specifies the model file format.
src_subword_type: sentencepiece
src_subword_model: '/home/project/wasm-type-prediction/snowwhite/subword/wasm/500.model'
# Specific arguments for pyonmttok
# This tokenizer runs _before_ SentencePiece/BPE. Since we don't want additional
# tokenization besides the ones by SPM, use mode: none.
src_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

tgt_subword_type: sentencepiece
tgt_subword_model: '/home/project/wasm-type-prediction/snowwhite/subword/wasm/500.model'
tgt_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

# We don't want any "dynamic" tokenization, like unigram sampling, so disable:
# Number of candidates for SentencePiece sampling
# src_subword_nbest: 20
# Smoothing parameter for SentencePiece sampling
# src_subword_alpha: 0.1

# Corpus opts:
data:
    corpus_1:
        path_src: '/home/project/wasm-type-prediction/data/train/wasm.txt'
        path_tgt: '/home/project/wasm-type-prediction/data/train/type.txt'
        # Not sure what the difference is between sentencepiece and onmt_tokenize
        # transforms: [sentencepiece, filtertoolong]
        transforms: [sentencepiece, filtertoolong]
        weight: 1

    valid:
        path_src: '/home/project/wasm-type-prediction/data/dev.10000/wasm.txt'
        path_tgt: '/home/project/wasm-type-prediction/data/dev.10000/type.txt'
        transforms: [sentencepiece, filtertoolong]

#################################################
# onmt_train options:
#################################################
# Voca
share_vocab: True

# Subword
src_subword_model: "../snowwhite/subword/wasm/500.model"
tgt_subword_model: "../snowwhite/subword/type/500.model"
src_subword_nbest: 1
src_subword_alpha: 0.0
tgt_subword_nbest: 1
tgt_subword_alpha: 0.0

update_vocab: true
train_from: "./save/nllb-200-1.3Bdst-onmt.pt"
reset_optim: all
save_data: "./"
save_model: "./save/nllb-200-wasm"
log_file: "train.log"
save_checkpoint_steps: 1000

# Parameters
seed: 1234
report_every: 50
train_steps: 50000
valid_steps: 1000
early_stopping: 10  # 'NoneType' object has no attribute 'ppl' error 발생 -> trainer.py valid_stats = None -> valid_iter = None

# Batching
bucket_size: 262144
num_workers: 4
prefetch_factor: 400
world_size: 1
gpu_ranks: [0]
batch_type: "tokens"
batch_size: 128
valid_batch_size: 128
# batch_size_multiple: 1
accum_count: [32, 32, 32]
accum_steps: [0, 15000, 30000]

# Optimization
model_dtype: "fp16"
#apex_opt_level: "O2"
optim: "fusedadam"
learning_rate: 2
warmup_steps: 4000
decay_method: "noam"
adam_beta2: 0.98
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
override_opts: true
encoder_type: transformer
decoder_type: transformer
enc_layers: 24
dec_layers: 24
heads: 16
hidden_size: 1024
word_vec_size: 1024
transformer_ff: 8192
add_qkvbias: true
add_ffnbias: true
dropout_steps: [0, 15000, 30000]
dropout: [0.1, 0.1, 0.1]
attention_dropout: [0.1, 0.1, 0.1]
share_decoder_embeddings: true
share_embeddings: true
position_encoding: true
position_encoding_type: 'SinusoidalConcat'
