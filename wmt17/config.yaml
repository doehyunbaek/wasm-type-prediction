#################################################
# onmt_build_vocab options:
#################################################

## Where the samples will be written
save_data: save/data
save_model: save/model
save_checkpoint_steps: 1000
## Where the vocab(s) will be written
src_vocab: 'vocab/src'
tgt_vocab: 'vocab/tgt'
# Prevent overwriting existing files in the folder
overwrite: False

# NOTE Use sentencepiece even if SPM model is BPE, since this just specifies the model file format.
src_subword_type: sentencepiece
src_subword_model: '/home/project/wasm-type-prediction/snowwhite/subword/wasm/500.model'
# Specific arguments for pyonmttok
# This tokenizer runs _before_ SentencePiece/BPE. Since we don't want additional
# tokenization besides the ones by SPM, use mode: none.
src_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

tgt_subword_type: sentencepiece
tgt_subword_model: '/home/project/wasm-type-prediction/snowwhite/subword/wasm/500.model'
tgt_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

# We don't want any "dynamic" tokenization, like unigram sampling, so disable:
# Number of candidates for SentencePiece sampling
# src_subword_nbest: 20
# Smoothing parameter for SentencePiece sampling
# src_subword_alpha: 0.1

# Corpus opts:
data:
    corpus_1:
        path_src: '/home/project/wasm-type-prediction/data/train/wasm.txt'
        path_tgt: '/home/project/wasm-type-prediction/data/train/type.txt'
        # Not sure what the difference is between sentencepiece and onmt_tokenize
        # transforms: [sentencepiece, filtertoolong]
        transforms: [onmt_tokenize]
        weight: 1
    valid:
        path_src: '/home/project/wasm-type-prediction/data/dev.10000/wasm.txt'
        path_tgt: '/home/project/wasm-type-prediction/data/dev.10000/type.txt'
        transforms: [onmt_tokenize]

share_vocab: True # from ende

# onmt_train options:
#################################################
# MUST USE CUDA_VISIBLE_DEVICES=1 to select second GPU

# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# multi-GPU training
# world_size: 2
# gpu_ranks: [0, 1] # or 1

valid_steps: 1000
train_steps: 50000 # default: 100000
log_file: 'train.log'
report_every: 50 # default 50
early_stopping: 10

optim: "fusedadam"
learning_rate: 2
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]

batch_size: 64
valid_batch_size: 64
batch_size_multiple: 8

encoder_type: transformer
decoder_type: transformer
enc_layers: 6
dec_layers: 6
heads: 16
hidden_size: 1024
word_vec_size: 1024
transformer_ff: 4096
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
share_decoder_embeddings: true
share_embeddings: true
position_encoding: true

src_seq_length_trunc: 96 # default None (don't truncate, filter out completely)
src_seq_length: 10000 # default 50, set to absurd high values such that no sample is filtered BEFORE truncation
tgt_seq_length_trunc: 96 # default None (don't truncate, filter out completely)
tgt_seq_length: 1000 # default 50, set to absurd high values such that no sample is filtered BEFORE truncation