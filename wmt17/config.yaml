#################################################
# onmt_build_vocab options:
#################################################

## Where the samples will be written
save_model: save/model
save_checkpoint_steps: 1000
## Where the vocab(s) will be written
src_vocab: 'vocab/src'
tgt_vocab: 'vocab/tgt'
# src_vocab_size: 36000
# tgt_vocab_size: 36000
# vocab_size_multiple: 8
# src_words_min_frequency: 2
# tgt_words_min_frequency: 2
# share_vocab: True
# n_sample: 0

# src_seq_length_trunc: 500 # default None (don't truncate, filter out completely)
# src_seq_length: 10000 # default 50, set to absurd high values such that no sample is filtered BEFORE truncation
# tgt_seq_length_trunc: 500 # default None (don't truncate, filter out completely)
# tgt_seq_length: 1000 # default 50, set to absurd high values such that no sample is filtered BEFORE truncation

# Prevent overwriting existing files in the folder
overwrite: False

# NOTE Use sentencepiece even if SPM model is BPE, since this just specifies the model file format.
src_subword_type: sentencepiece
src_subword_model: '/home/project/wasm-type-prediction/snowwhite/subword/wasm/500.model'
# Specific arguments for pyonmttok
# This tokenizer runs _before_ SentencePiece/BPE. Since we don't want additional
# tokenization besides the ones by SPM, use mode: none.
src_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

tgt_subword_type: sentencepiece
tgt_subword_model: '/home/project/wasm-type-prediction/snowwhite/subword/wasm/500.model'
tgt_onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

# We don't want any "dynamic" tokenization, like unigram sampling, so disable:
# Number of candidates for SentencePiece sampling
# src_subword_nbest: 20
# Smoothing parameter for SentencePiece sampling
# src_subword_alpha: 0.1

# Corpus opts:
data:
    corpus_1:
        path_src: '/home/project/wasm-type-prediction/data/train/wasm.txt'
        path_tgt: '/home/project/wasm-type-prediction/data/train/type.txt'
        # Not sure what the difference is between sentencepiece and onmt_tokenize
        # transforms: [sentencepiece, filtertoolong]
        transforms: [onmt_tokenize]
        weight: 1
    valid:
        path_src: '/home/project/wasm-type-prediction/data/dev.10000/wasm.txt'
        path_tgt: '/home/project/wasm-type-prediction/data/dev.10000/type.txt'
        transforms: [onmt_tokenize]

share_vocab: True # from ende

# onmt_train options:
#################################################
# MUST USE CUDA_VISIBLE_DEVICES=1 to select second GPU

# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# multi-GPU training
# world_size: 2
# gpu_ranks: [0, 1] # or 1

valid_steps: 1000
train_steps: 50000 # default: 100000
log_file: 'train.log'
report_every: 50 # default 50
early_stopping: 10

model_dtype: "fp16"
#apex_opt_level: "O2"
optim: "fusedadam"
learning_rate: 2
warmup_steps: 4000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

bucket_size: 262144
num_workers: 4
prefetch_factor: 400
world_size: 1
gpu_ranks: [0]
batch_type: "tokens"
batch_size: 128
valid_batch_size: 128
# batch_size_multiple: 8
accum_count: [10]
accum_steps: [0]

encoder_type: transformer
decoder_type: transformer
enc_layers: 6
dec_layers: 6
heads: 16
hidden_size: 1024
word_vec_size: 1024
transformer_ff: 4096
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
share_decoder_embeddings: true
share_embeddings: true
position_encoding: false